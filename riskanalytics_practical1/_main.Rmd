---
title: "Practical 1"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


In this practical, we will analyse data pertaining to customer waiting times in a governmental office. The data provided to us is the $waiting.csv$ file are daily average waiting times from *January 3, 2017* to *October 26, 2018.*

Our goal is to model and monitor the time customers spend waiting, and to take action if the service quality deteriorates too badly, i.e. if waiting times are unacceptably high.

## Exercise A {.tabset .tabset-fade .tabset-pills}

Read in the data and plot the daily waiting times. Compute and display the five-number summary (boxplot) of the daily waiting times for each weekday. What do you observe?

### Daily plot {-}

```{r daily-plot, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

library(dplyr)
library(ggplot2)
library(broom)

#loading the data
data <- read.csv(here::here("waiting.csv"))
data$Date<-as.Date(data$Date)

####################option 2 plot

mean_wt <- mean(data$Average.Wait.Seconds )

data %>% 
  ggplot2::ggplot(aes(Date, Average.Wait.Seconds, group = 1, cex.names=0.1)) + 
                    geom_line()+
                    theme_bw()+
                    scale_x_date(limits = as.Date(c("2017-01-03","2018-10-26"))) +
                    labs(x = "Date", y = "Average Wait (Seconds)",  
                         title = "Daily Plot")+
                    theme(plot.title = element_text(face = "bold", hjust = 0.5)) +
                    theme(axis.text.x = element_text(angle = 0, hjust = 0.5, size = 8)) +
                    theme(axis.text.y = element_text(hjust = 0.5, size = 8)) + 
                    geom_hline(yintercept = mean_wt,
                               color = "red",
                               linetype = "solid") + 
                    theme(plot.subtitle = element_text(face = "italic")) +
                    labs(subtitle = "Mean is represented as a red solid line at 441 seconds ")

rm(mean_wt)

```
  
### Boxplot {-}

<div class = "row">
<div class = "col-md-6">

```{r , echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

##############################showing the five-number summary of all the data 
#five-number summary of waiting time per weekday

summary5_weekday<- data %>% 
                      group_by(Weekday) %>% 
                      summarise(Min = min(Average.Wait.Seconds),
                                Q1 = quantile(Average.Wait.Seconds, 0.25), 
                                Mean = round(mean(Average.Wait.Seconds),0), 
                                Q3 = quantile(Average.Wait.Seconds, 0.75), 
                                Max = max(Average.Wait.Seconds))

###########Summary table###############################

kableExtra::kable(summary5_weekday, caption = "Five-number summary by weekday")%>%
  kableExtra::kable_styling(bootstrap_options = c("striped", full_width = FALSE, position="left")) 



```
</div>

<div class = "col-md-6">

```{r , echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

#changing the order for the labels in the axis x for the boxplot per weekday

data$Weekday <- factor(data$Weekday, levels = c('Monday', 'Tuesday', 'Wednesday',
                                                'Thursday','Friday' ))

#plotting the boxplot per weekday
p<-data %>% 
    ggplot(aes(Weekday, Average.Wait.Seconds, fill = Weekday)) +
    geom_boxplot()+
    theme_bw()+
    labs(x = "Day", y = "Average Wait (Seconds)",  title = "Boxplot by weekday")+
    theme(plot.title = element_text(face = "bold", hjust = 0.5)) +
    theme(axis.text.x = element_text(angle = 0, hjust = 0.5, size = 10)) +
    theme(axis.text.y = element_text(hjust = 0.5, size = 8))

p


rm(summary5_weekday, p )

```
</div>
</div>

## {.toc-ignore}


$Conclusion$ : our main observations regarding the charts below are the following:

- The daily plot chart below show a continue oscillation of the data during the 447 observations.
- There are high deviation values regarding the mean in each day.
- The average per weekday are similar within them, nevertheless, the highest average is made in friday.
- The highest waiting time is on wednesday.


## Exercise B {.tabset .tabset-fade .tabset-pills}

Using a normal approximation produce an upper control limit or confidence line for the waiting times at the one-year return level. What do you notice? Using a Q-Q plot, comment on whether the Normal model is appropriate.

### Upper Control Limit {-}

```{r ucl, echo=TRUE, message=FALSE, warning=FALSE, paged.print=FALSE}

########Assuming that incomes are normally distributed

#95% CI--> alpha = 0.05 We can get z(alpha/2) = z(0.025) from R:
nrow<- nrow(data)
qnorm<- qnorm(.975)   #1.959964
meanData<- mean(data$Average.Wait.Seconds) #440.6398
sdData<- sd(data$Average.Wait.Seconds)

###########Our margin of error is 

upper_control_limit<- mean(data$Average.Wait.Seconds)+
                            qnorm*(sdData/sqrt(nrow))

```
  

```{r ucl-plot, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

##########not need of the lower bound#######################

#lb<- mean(data$Average.Wait.Seconds)-
#      qnorm(0.975)*(sd(data$Average.Wait.Seconds)/sqrt(nrow(data)))

###########################################v

data %>% 
  ggplot(aes(Date, Average.Wait.Seconds, group = 1, cex.names=0.1)) + 
  geom_line()+
  theme_bw()+
  scale_x_date(limits = as.Date(c("2017-01-03","2018-10-26"))) +
  labs(x = "Date", y = "Average Wait (Seconds)",  
       title = "Daily Plot" )+
  theme(plot.title = element_text(face = "bold", hjust = 0.5)) +
  theme(axis.text.x = element_text(angle = 0, hjust = 0.5, size = 8)) +
  theme(axis.text.y = element_text(hjust = 0.5, size = 8)) +
  geom_hline(aes(yintercept=upper_control_limit), 
             colour="red", 
             linetype="solid") +
  labs(subtitle = "The Upper Control Limit is represented as a red solid line at 460 seconds")+
  theme(plot.subtitle = element_text(face = "italic"))


```
  
### Probability One-year-return level {-}

```{r o-y-rl, echo=TRUE, message=FALSE, warning=FALSE, paged.print=FALSE}

###Probability to exceed the upper bounds

#If X~N(μ, σ)--> pnorm(x, μ, σ) function to calculate P(X > upperbound).

probability_exceed_UCL<- 1 - pnorm( upper_control_limit, 
                         meanData, sdData)

probability_exceed_UCL

#1/p_exceed_ub = m-years return level


```

### QQ-Plot {-}


```{r qq-plot, echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}

########################the normal approximation

qqnorm(data$Average.Wait.Seconds, pch = 1, frame = FALSE, 
       main = "Q-Q plot")
qqline(data$Average.Wait.Seconds, col = "steelblue", lwd = 2)

#as you can see the data is not normal is right-skewed (or positively skewed) because we see the upper end of the Q-Q plot to deviate from the straight line and the lower and follows a straight line then the curve has a longer till to its right 

rm(upper_control_limit, meanData, nrow, qnorm, sdData, probability_exceed_UCL)

```


## {.toc-ignore}

$Conclusion$: our main observations regarding the analisis are:

- The upper bound with a normal approximation is very close the mean of the waintin time (Upper bound= 460, Mean= 441).
- The probability that the upper bound won't be exceeded during the 1-year return period is quite high, because the upper bound do not considere the extreme values and it tends to concentrate in the middle due to the normal approximation.
- In the QQ-plot we can confirm that the data is not normal is right-skewed (or positively skewed) due to the desviation of the straight line in both extrems. 


## Exercise C

The aim of this analysis is to focus on negative effects (long wait times). Explain how you would proceed using 1) a block maxima and 2) a peaks-over-threshold approach. For each method, carry out the required data aggregation and transformation.

### Block Maxima approach {.tabset .tabset-fade .tabset-pills}

We have proposed the following steps:

- First, we determine the block size and the maximum valor for each of them. For this exercise we choose weekly blocks.  

`Block Maxima:` $$M_{i}=max\{X_{(i−1)n+1},\cdots,X_{in}\},i=1,⋯,m$$

- Then, we fit the a Generalized extrem Value, assuming that it follows \(GEV(\mu_{n},\sigma_{n},\xi)\) distribution and we will focus on maximize the log-likelihood.  

- In this step, we decide to test if the parameters correspond to the Gumbel distribution. 

- Finally, we asses the model and compute m-year return levels.

#### Divide data {-}

```{r ,include=FALSE, warning=FALSE, message=FALSE, echo=FALSE}

# load required packages
library(extRemes)
library(distillery)
library(xts)
library(evd)

```

```{r bm1, warning=FALSE, message=FALSE, echo=TRUE}

#we should first extract the maximum temperature values-->  block maxima.

#week variable creation
# derive AMS for maximum precipitation

data_ts <- xts::xts(data$Average.Wait.Seconds, order.by = data$Date) #time series creation
wms_data <- xts::apply.weekly(data_ts, max) #create the weekly maximal serie


```

#### Plot {-}

```{r plotbm, message=FALSE, warning=FALSE, echo=FALSE}

##############plot####################

data_ts_dataframe<- data.frame(as.matrix(data_ts), Date=time(data_ts))
wms_data_frame<- data.frame(as.matrix(wms_data), Date=time(wms_data))

names(data_ts_dataframe)<-c("Average.Wait.Seconds", "Date")
data_ts_dataframe$Date<- as.Date(data_ts_dataframe$Date)
names(wms_data_frame)<-c("Average.Wait.Seconds", "Date")
wms_data_frame$Date<- as.Date(wms_data_frame$Date)


ggplot() + 
  geom_line(data = data_ts_dataframe, aes(x = Date, y = Average.Wait.Seconds), color = "black") +
  geom_point(data = wms_data_frame , aes(x = Date, y = Average.Wait.Seconds), color = "red") +
  theme_bw()+
  scale_x_date(limits = as.Date(c("2017-01-03","2018-10-26"))) +
  labs(x = "Date", y = "Average Wait (Seconds)",  
       title = "Weekly division",
       subtitle ="" )+
  theme(plot.title = element_text(face = "bold", hjust = 0.5)) +
  theme(axis.text.x = element_text(angle = 0, hjust = 0.5, size = 8)) +
  theme(axis.text.y = element_text(hjust = 0.5, size = 8))

rm(data_ts_dataframe, wms_data_frame)

#According to the Fisher–Tippett–Gnedenko theorem, the distribution of block maxima can be approximated by a generalized extreme value distribution.

```


#### Fit to GEV {-}

```{r distributionBM , echo=TRUE, warning=FALSE, message=FALSE}

#According to the Fisher–Tippett–Gnedenko theorem, the distribution of block maxima can be approximated by a generalized extreme value distribution.

gev.fit <- ismev::gev.fit(as.vector(wms_data),show = F)
gev.fit2 <- extRemes::fevd(as.vector(wms_data),method = "MLE", type = "GEV")

```

#### Testing Parameters {-}

<div class = "row">
<div class = "col-md-6">

```{r table_parameters , echo=FALSE, warning=FALSE, message=FALSE}

#######################summary table#####################

summary_table <- rbind.data.frame(gev.fit2$results$par, unname(gev.fit$se) )
summary_table <- round(summary_table, 2)

colnames(summary_table ) <- c("Location", "Scale", "Shape")
rownames(summary_table ) <- c("Estimates", "Std.errors")

kableExtra::kable(summary_table, caption = "This table show the parameters of the GEV fit model")%>%
  kableExtra::kable_styling(bootstrap_options = c("striped", 
                                                  full_width = FALSE, position="left")) 


```
</div>

<div class = "col-md-6">
```{r test-distribution, echo=TRUE, warning=FALSE, message=FALSE}

######Testing the Gumbel distribution (ξ=0) hypothesis with LRT#####################
#################################################################################

gev_gumb <- fevd(as.vector(wms_data), type="Gumbel", units="deg C")

test_gumbel<- lr.test(gev_gumb,gev.fit2)

test_gumbel

#leads us to not reject the Gumbel hypothesis !
########################################################################


```
</div>
</div>


#### Plot {-}

```{r bmfit-plot , echo=FALSE, warning=FALSE, message=FALSE, fig.asp=1.5}

rm(summary_table, gev_gumb, test_gumbel)

################## diagnostic PLOTS
plot(gev.fit2)

# return levels:
rl_gev <- return.level(gev.fit2, conf = 0.05, return.period= c(2,5,10,20,50,100))

return_level_gev <- c(  rl_gev[1], rl_gev[2], rl_gev[3] , rl_gev[4], rl_gev[5], rl_gev[6] )
return_period<- c("2 years", "5 years","10 years", "20 years","50 years", "100 years")
return_level_gev<- data.frame(return_period, return_level_gev)

```

### {.toc-ignore}


$Conclusion$ : our main observations are the following:

- Even if the shape value is negative it is small enough to fit a Gumbel distribution.



### Peaks-over-theshold approach {.tabset .tabset-fade .tabset-pills}

We have proposed the following steps:

- First, we need stablish a larga enough treshold \(u\), where 
$$\mathbb{P}\left(X_{i}-u>y|X_{i}>u\right)$$, for being able to do we apply a mean residual plot a we select the threshold when the plot start looks linear.

- Then, we fit the data to a Generaliaze Pareto distribution.  

- Finally, we asses the model and compute m-year return levels.

#### Setting the threshold{-}

```{r setting-threshold, warning=FALSE, message=FALSE, echo=FALSE}

# The idea is to ﬁnd the lowest threshold where the plot is nearly linear;
mrlplot(as.vector(data_ts), main="Mean Residual Daily Plot")
#around 600 start to be linear
threshold <- 600


```

#### Plot data {-}

```{r threshold-plot, warning=FALSE, message=FALSE, echo=FALSE}

plot(x = rep(1:447, each = n), y = unlist(data_ts), main = "Peak Over Thresholds",
     sub = paste("threshold =", threshold), xlab = "series", ylab = "value")
abline(h = threshold, col = "red")

```

#### Fit GP {-}

```{r fit-gp, warning=FALSE, message=FALSE, echo=TRUE}

# maximum likelihood estimation
pot.fit <- fevd(as.vector(data_ts), method = "MLE", 
                type="GP", 
                threshold = threshold)

```

#### Plot GP {-}

```{r gp-plot, fig.asp=1.5 , warning=FALSE, message=FALSE, echo=FALSE}

# diagnostic plots
plot(pot.fit)

########compare return_level values##############################

rl_pot <- return.level(pot.fit, conf = 0.05, return.period= c(2,5,10,20,50,100))

return_level_pot1 <- c(  rl_pot[1], rl_pot[2], rl_pot[3] , rl_pot[4], rl_pot[5], rl_pot[6] )
return_period<- c("2 years", "5 years","10 years", "20 years","50 years", "100 years")
return_level_pot<- data.frame(return_period, return_level_pot1)
return_level_pot

compare_return_level<- data.frame(return_level_gev, return_level_pot1)


```

### {.toc-ignore}


## Exercise D

Propose a model for the data you have processed in the previous question. Make sure to justify your model choice using residuals and goodness-of-fit tests. (Hint: you may use the evd package.)

Next,  we use likelihood-ratio test for two nested extreme value distribution models, the function used comes from the *extRemes package*.

```{r test fit, warning=FALSE, message=FALSE, echo=TRUE}

extRemes::lr.test(pot.fit, gev.fit2) #gev. fit2 better

kableExtra::kable(compare_return_level)%>%
  kableExtra::kable_styling(bootstrap_options = c("striped")) 



```


We can confirm by the test and, also, by the AICs that the **Peaks-over-threshold approach** is the best one for fitting the data.

## Exercise E {.tabset .tabset-fade .tabset-pills}

Finally, use your model to derive an upper control limit or confidence line for the waiting times at the one-year return level.

### Computing CI{-}

```{r selected CI, warning=FALSE, message=FALSE, echo=FALSE}

#we need to assure the following trade-off: threshold too low – bias because of the model asymptotics being invalid; threshold too high – variance is large due to few data points.


ci(pot.fit, return.period = 2, verbose = T)  #check the range here

CI_prof_pot <- ci(pot.fit, method="proflik", xrange = c(900, 1700),  
                    return.period = 2, verbose = TRUE)

```

### Plots Bounds {-}

```{r plot-bound, warning=FALSE, message=FALSE, echo=FALSE}

#lets us add the bounderies
hist(as.vector(data_ts), 2, col = "lightblue",
     xlim = c(400, 2500), prob = T, ylim = c(0, 0.002),
     xlab = "annual max (0.01 in)",
     main = "95% CI for 2-yr RL")
xg <- seq(0, 1700, len = 1000)
mle <- pot.fit$results$par
lines(xg, dgpd(xg, loc = 600,
               scale = mle[1], shape = mle[2]))
for (i in 1:3) abline(v = CI_prof_pot[i], lty = 3, col = "red")
legend("topleft", legend = c( "Prof"),
       col = c("red"), lty = c(2, 3))


```

### Probability 1-year return {-}

```{r probability 1-y , warning=FALSE, message=FALSE, echo=TRUE}

#lets us add the bounderies

return_2year <- return.level(pot.fit, conf = 0.05, return.period= c(2))
probability_1_year_return <- 1/(return_2year[1]/2)

probability_1_year_return

```

## {.toc-ignore}

$Conclusion$ :

- Given our threshold $u$= 1293.76 the probability of exceed it is 1.54%. 

<!--chapter:end:Practical_1.Rmd-->

---
title: "Practical 2"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, include = FALSE}

library(dplyr)
library(xts)
library(magrittr)
library(MASS)
library(extRemes)
library(gPdtest)

```

In this second practical, we will look at product sales data belonging to a large American retailer. 

As the person in charge of re-stocking products after they are sold, you would like to avoid a ‘stock-out’, that is, when on a given day there are not enough products to satisfy customer demand.

Consider the observations recorded in sales 1.txt, which contain the daily sales volumes of Product 1 over n = 1913 days.

# Exercise a
> Construct a figure which shows the daily sales volume, and study the values just before the product is out of stock. What do you notice? Explain why this is related to stock-out.

```{r loading data, include = FALSE, warning=FALSE}

sales <- read.delim(here::here("sales_1.txt"), header = FALSE)
sales %<>% dplyr::rename(Sales = V1)

```


```{r daily sales, fig.width=20, fig.asp=0.2}

plot(sales$Sales, type = "l", main = "Daily Sales")
outofstock <- which(diff(sales$Sales) == -sales$Sales & sales$Sales !=0)
abline(v = outofstock, col = "red")

sales %>% dplyr::mutate(date = as.Date(1:1913)) %>% dplyr::filter(Sales == 0)

```
It seems there is some seasonality element in the data, as the product is out of stock alomsto every year at the same time (around the end of November for almost every year), hence it could be avoided as it is repeated thorugh different years. 

# Exercise b

> Assume Product 1 is a perishable good sold at a constant price of 10.– over the time period considered. Items of Product 1 are produced at cost 1.– and items which are unsold can be salvaged at 10% of cost, that is, at value s = 10 ct. Suppose that the sales of Product 1 are distributed according to F. Using the newsvendor model, give a formula for the critical fractile q = F−1(p).

Otherwise, we can calculate it by hand. 

```{r manual newsvendor}

#price
pr <- 10
#cost of production
c <- 1 
#salvage cost
s <- 0.1
#cost of shortage 
cs <- pr-c
#cost of overage 
co <- c-s
#critical fractile
p <- cs / (cs + co)

```

In this case, the formula to calculate the critical fractile is the following:

$$ q = F^-1 (p)$$ 

is equal to 

$$ q = F^{-1} (\frac {cs}{cs + co}) $$ 

The cs as the cost of shortage, which corresponds to the cost per unit it would incur if there is still demand but we are out of stock, hence a missed sale which correspond to the price minus the costs (p - c). 
On the other hand co is the cost of overage, which is the cost per unit of having too many products compared to the demand, hence a product produced and unsold which is the cost of production minus the salvage cost (c - s). 

#Exercise c

> Explain how the critical fractile is related to the Value at Risk, and give an interpretation of the Expected Shortfall at the level p.

The **critical fractile**, balances the cost of being understocked and the total costs of being either overstocked or understocked, and it gives the optimal quantity that should be produced to minimizes the costs. It's the quantile which optimize the probability that the demand is lower than the quantity Q that has been ordered.

The **Value at Risk** (VaR) denotes a quantile of the distribution, which is  the smallest number such that the random variable X that we are studying exceeds a value x with probability lower than 1 - alpha. In other words, with probability alpha, the variable X will be smaller than VaR. 
Hence, we are looking at the heavy tails of the distribution of the variable. 

The interest is when the newsvendor model gives a high quantile to look at (e.g. above 0.9, like in our case in the previous question). What we want to avoid is to go beyond the critical fractile, because in that case we will not choose the optimal quantity, and we will increase the costs and hence decrease the profit, even though this will happen with a low probability (of 1 - q).
This is linkable to the Value at Risk as it is also describing an extreme situation in which the variable takes a value that is higher than a certain threshold, and, as we are in the risk analysis field we are studying the losses on investments, we want to avoid these extreme cases, as they mean a high loss. 

The **Expected Shortfall** at level alpha is the average value of X when it is higher than VaR. It is related to VaR by 

$$ ES_\alpha (x) = \frac{1}{1 - \alpha} \int_\alpha^{1}q_u (F_X)du =  \frac{1}{1 - \alpha} \int_\alpha^{1} VaR_\beta(L)d\beta  $$

The Expected Shortfall at level p will then be the value of the sales if they are higher then the VaR, so for the level p, the ES will be the average value of the sales that are higher than the level p that is calculated as the quantile of the critical fractile q.

# Exercise d

> Fit a Poisson model to the sales volume data by assuming that Yi ∼ Poisson(μ) and using maximum likelihood estimation for μ. (Hint: you may use MASS::fitdistr). According to this model, what is the estimated critical fractile?

```{r poisson by hand}

poisson <- fitdistr(sales$Sales, "Poisson")
#lambda of the poisson distribution
lambda <- poisson$estimate
#quantile 
q <- qpois(p, lambda)

```

In the case of a poisson distribution, we find a critical fractile by using the following formula: 

$$ q = F^{-1} (p) = \sum_{i = 0}^p \frac {e{-\lambda}\lambda^i}{i!}$$

And it corresponds to `r q`. 

#Exercise e

> Suppose we model the sales as an extreme value distribution using a peaks-over-thresholds method. Give the Value at Risk at level p under this model.

```{r EVD, fig.width=12}

#Mean Residual Life Plot
extRemes::mrlplot(sales$Sales, main="Mean Residual Life Plot")
#Finding and appropriate threshold 
extRemes::threshrange.plot(sales$Sales, r = c(70, 150), nint = 80)

#let's fix the threshold at 120 as after it the distribution seems to be flat in the second graph and there is the elbow in the first one
u <- 120

plot(sales$Sales, type = "l", main = "Daily Sales")
abline(h = u, col = "red")

#Weights for the observations above the threshold chosen
W <-  sales %>% dplyr::filter(Sales > u) %>% dplyr::mutate(W = Sales - u) %>% dplyr::select(W)

#fitting a GPD, method amle as we want the shape parameter positive
gpd <- gPdtest::gpd.fit(W$W, method = "amle")

#extracting the parameters 
#shape
beta <- gpd[1,]
#scale
xi <- gpd[2,]

#setting the alpha for the VaR to the level we have calculated 
alpha <- p

#calculating Fbar with the approximation 
Fbar <- nrow(W) / nrow(sales)

#calculating the values at risk 
VaR <- (u + (beta/xi) * ( ( ( (1 - alpha) / (Fbar) ) ^ (-xi) ) - 1) )

```

We use the first two graphs to determine the therhsold that we want to use. What we can see in the first one is that between the value 100 and 150 the curve seems to flatten, which is confirmed by the second graphs, as the parameters are becoming constant already from before 100. 

We decide to be safe and to choose a threshold of 120, as we believe that after that the Mean Residual Life Plot shows a flatter line. 

We move on by calculating the weights of the observations that are above this threshold (that are the points in the graph above the red line). 

Then, using these observations we fit a GPD and we estimate the parameters, that we will insert in the formula to calculate the VaR.

The resulting value is `r VaR`. 

# Exercise f

> Perform a binomial back-test over the last 300 days in the dataset for the models in d) and e), using a window size of 365 days. What are your conclusions?

To backtest:
1. Split the data and create a test set including the last 300 observations of the dataset
2. Calculate the VaR for each one of the observations inside the test set using a window of 365 days 
3. Compare the estimated VaR to the real values and see if there is any violation, meaning that the real value is higher than the VaR for a given day
4. Consider that the violations follow a binomial distribution(300, 1 - p), test the hypothesis that the observed proportion of violations over the test set is different from the expected proportion given by the binomial distribution with a binomial test

## 1. Test set

We will split the data in two subgroups, the first one being the training set which we will use to estimate the VaR will comprehend the observations up to the last 300 days of the data, while the last 300 observations will be the test set to which we will compare the VaR we will find. 

```{r test set}

salesTest <- sales$Sales[(nrow(sales)-299):(nrow(sales))]

```

## 2. Calculate VaR 

Then we will calculate the VaR for the test set using a moving window of 365 days. 

In order to do so, we start by creating a function to calculate the VaR for a given window, which works in the following way:
- same thershold we decided before (hence 120)
- window of 365 days before the observations that we want to estimate
- calculate the weights for the observations in this window which are above the threshold by taking the differences between the sales and the thershold
- fit a GPD on the weights and estimate the parameters for the shape and the scale
- same alpha as before, which is the p value we calculated
- estimation of Fbar ny dividing the number of observations above the threshold in the window and the total number of observation in the window 
- put all the estimators in the formula to calculate the value at risk

```{r VaR function}

VaRest <- function(df){
  
  #threshold
  u <- 120
  
  #Weights for the observations above the threshold chosen
  W <-  df %>% dplyr::filter(Sales > u) %>% dplyr::mutate(W = Sales - u) %>% dplyr::select(W)
  
  #fitting a GPD, method amle as we want the shape parameter positive
  gpd <- gPdtest::gpd.fit(W$W, method = "amle")
  
  #extracting the parameters 
  #shape
  beta <- gpd[1,]
  #scale
  xi <- gpd[2,]
  
  #setting the alpha for the VaR to the level we have calculated
  alpha <- p
  
  #calculating Fbar with the approximation 
  Fbar <- nrow(W) / 300
  
  #calculating the values at risk 
  (u + (beta/xi) * ( ( ( (1 - alpha) / (Fbar) ) ^ (-xi) ) - 1) )
  
  }

```

Then we move on with the calculation of the value at risk for each observation in the test set. 
In order to do it, we will first create a vector to store the values, which will have a length of 300 as the number of observations in the test set. 
Then, we create a loop for the 300 observations, we want to create the window considered to calculate the value at risk by including the 365 observations that precedes the one for which we want to estimate the value at risk.
Eventually, we get the different values at risk for each one of the observations. 

```{r}
#create a vector to store the values 
VAR <- as.numeric(1:300)

for(i in 1:300){
  #select window 
  wind <- sales %>% dplyr::slice((1913-i):(1913-i-365))
  #calculate VaR
  VAR[i] <- VaRest(wind)
}

VAR

```

We can see that there are quite a lot of repetitions in the estimation of the values at risk, this is given by the fact that the the observations contained in the windows of adjacent dates are all the same but one, as we discard the oldest one to replace it with the real value of the observation of the day before the one we are considering. Hence, if none of these two values is describing a violation (meaning that it is higher than the threshold that we have chosen), the computation of the value at risk won't be affected, as all the paramters that we use to estimate the VaR will remain unchanged: the weights will be the same (since we consider only the values that are above the threshold) and so will be the GPD and its parameters' estimations, the fbar is also the same as Nu stays the same and the alpha is fixed.

## 3. Violations 

Now, we compare the VaR we estimated to the real values, to see how many violations there are. The should be following a bernoulli distribution with n = 300 and p = 1 - alpha.

```{r comparison}

#number of observed violations
violations <- sum(ifelse(VAR > salesTest, 0, 1))
#number of theoretical violations, given by the mean of the bernoully distribution
ev <- 300*(1 - p)

```

We find out that there are `r violations` violations. 

We can compare this value to the theoretical one that we expect to get, which is `r ev`.

## Binomial test

We can test whether they are statistically significantly different by using a binomial test, which has the following characteristics: 

$$H_0: \pi = \pi_0$$
In our case the theoretical π will equal to `r 1 - p`, while the probability found in the observations is `r violations/300`. We want to test if waht it observed is different from the expected value. 

```{r binomial test}

(bintest <- binom.test(45, 300, p = 1-p))

```

We can see that the p-value is quite low (`r bintest$p.value`), more specifically is lower than the significance level alpha = 5%, hence we cannot reject the null hypothesis of the two proportions being the same and hence that the distribution of the violations in the windows considered follows a binomial(300, 1-p).

<!--chapter:end:Practical_2.Rmd-->

---
title: "Practical 3"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(here)
sales10 <- read.csv(here::here("sales_10.csv"))
sales10adj <- read.csv(here::here("sales_10_adjusted.csv"))
```

> In this third practical, we expand the retail sales dataset of Practical 2 to include multiple products. 
Data for Product 1 (from Practical 2) and an additional 9 products are given in two forms: sales 10.csv and sales 10 adjusted.csv, with the same format. 
As the name indicates, sales volumes in the latter file have been adjusted for trend and seasonality (removing growth and cyclical patterns).

> (a) Explain the challenges involved when analysing the risk of stock-out across several products. What is the danger of considering each series independently?

Analyzing the risk of stock-out for various products is a hard task, because we might face different products' distributions, different underlying patterns (such as a difference in seasonality) and effects stemming from different external factors (the launch of a competitor's products phagocytizing the sales our one of the products, e.g). Multicolineraity is the reverse problem: products are too similar. In both cases, the fitted model for the stock-outs of the 10 products would not fit well the data, resulting in a potential poor information delivery and low accuracy for prescriptive actions.


Thus we assume that each series share a certain amount of information among them. In facts, we know here that the analysis would concern 10 products sold by the same company. Imagine those 10 products are very differents: they would necessary come from related production, similar distribution channel, inventories, sold to customers with more or less the same profile, since sharing the same needs, and so on.

Those elements illustrate those 10 products share "information". We cannot rely on a complete independance among products and this is the reason why we cannot consider each series as independant.We draw a plot without distinguing the product, but we see that some products follow the same pattern (obviously seen with the blue and orange curves).

```{r plot glimpse on product, echo=FALSE}
plot(sales10adj$Product_1[1:50], , type = "l", col = "blue", main = "Daily Sales",xlab = "Day", ylab = "Daily sales (unit)") +
lines(sales10adj$Product_2[1:50],  type = "l", col = "green") +
lines(sales10adj$Product_3[1:50], , type = "l", col = "purple") +
lines(sales10adj$Product_4[1:50], , type = "l", col = "red") +
lines(sales10adj$Product_5[1:50], , type = "l", col = "grey") +
lines(sales10adj$Product_6[1:50], , type = "l", col = "brown") +
lines(sales10adj$Product_7[1:50], , type = "l", col = "orange") +
lines(sales10adj$Product_8[1:50], , type = "l", col = "pink") +
lines(sales10adj$Product_9[1:50], , type = "l", col = "black") +
lines(sales10adj$Product_10[1:50], , type = "l", col = "yellow") 
```
To investigate quickly if our products are have any relationship, we compute the correlation matrix, as follow:

```{r kable correlation matrix, echo=FALSE}
sales10adj %>% cor(method = "pearson") %>% round( digits = 3) %>% kable() %>% kable_styling()
```

> (b) Propose one graphical and one numerical method of detecting dependence of extreme values of the demand across several products. Apply your chosen methods to the adjusted sales data and identify groups of related products (if any exist).

A graphical tool to distinguish 

```{r computation of the covariances set}
library(kableExtra)

```

> (c) In view of your answer to (f), would you apply a Gaussian copula model to these data?



> (d) (Hard) Fit a multivariate model to the adjusted sales data and estimate the 95% Demand at Risk for the sum:
$$S = X^{(1)} + X^{(2)} + ... + X^{(10)}$$
You can do so with the following steps: 

i. Transofrm the sales for each product to a uniform scale, that is, compute 
$$F_i(X^{(i)})$$
for each X in 1, ..., n being each of the product sales and F being the esimated Extreme Value distributions.


ii. Fit a multivariate copula of your choice using the `copula::fitCopula` function.


iii. Simulate from your fitted copula and transform the values back to their original scales (i.e. undo the transformation in i.)


iv. Compute the simulated values of S and their 95% Value at Risk.



<!--chapter:end:Practical_3.Rmd-->

