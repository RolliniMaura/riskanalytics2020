---
title: "Practical 2"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, include = FALSE}

library(dplyr)
library(xts)
library(magrittr)
library(MASS)
library(extRemes)
library(gPdtest)

```

In this second practical, we will look at product sales data belonging to a large American retailer. 

As the person in charge of re-stocking products after they are sold, you would like to avoid a ‘stock-out’, that is, when on a given day there are not enough products to satisfy customer demand.

Consider the observations recorded in sales 1.txt, which contain the daily sales volumes of Product 1 over n = 1913 days.

# Exercise a
> Construct a figure which shows the daily sales volume, and study the values just before the product is out of stock. What do you notice? Explain why this is related to stock-out.

```{r loading data, include = FALSE, warning=FALSE}

sales <- read.delim(here::here("sales_1.txt"), header = FALSE)
sales %<>% dplyr::rename(Sales = V1)

```


```{r daily sales, fig.width=20, fig.height=8}

plot(sales$Sales, type = "l", main = "Daily Sales")
outofstock <- which(diff(sales$Sales) == -sales$Sales & sales$Sales !=0)
abline(v = outofstock, col = "red")

sales %>% dplyr::mutate(date = as.Date(1:1913)) %>% dplyr::filter(Sales == 0)

```
It seems there is some seasonality element in the data, as the product is out of stock alomsto every year at the same time (around the end of November for almost every year), hence it could be avoided as it is repeated thorugh different years. 

# Exercise b

> Assume Product 1 is a perishable good sold at a constant price of 10.– over the time period considered. Items of Product 1 are produced at cost 1.– and items which are unsold can be salvaged at 10% of cost, that is, at value s = 10 ct. Suppose that the sales of Product 1 are distributed according to F. Using the newsvendor model, give a formula for the critical fractile q = F−1(p).

Otherwise, we can calculate it by hand. 

```{r manual newsvendor}

#price
pr <- 10
#cost of production
c <- 1 
#salvage cost
s <- 0.1
#cost of shortage 
cs <- pr-c
#cost of overage 
co <- c-s
#critical fractile
p <- cs / (cs + co)

```

In this case, the formula to calculate the critical fractile is the following:

$$ q = F^-1 (p)$$ 

is equal to 

$$ q = F^{-1} (\frac {cs}{cs + co}) $$ 

The cs as the cost of shortage, which corresponds to the cost per unit it would incur if there is still demand but we are out of stock, hence a missed sale which correspond to the price minus the costs (p - c). 
On the other hand co is the cost of overage, which is the cost per unit of having too many products compared to the demand, hence a product produced and unsold which is the cost of production minus the salvage cost (c - s). 

#Exercise c

> Explain how the critical fractile is related to the Value at Risk, and give an interpretation of the Expected Shortfall at the level p.

The **critical fractile**, balances the cost of being understocked and the total costs of being either overstocked or understocked, and it gives the optimal quantity that should be produced to minimizes the costs. It's the quantile which optimize the probability that the demand is lower than the quantity Q that has been ordered.

The **Value at Risk** (VaR) denotes a quantile of the distribution, which is  the smallest number such that the random variable X that we are studying exceeds a value x with probability lower than 1 - alpha. In other words, with probability alpha, the variable X will be smaller than VaR. 
Hence, we are looking at the heavy tails of the distribution of the variable. 

The interest is when the newsvendor model gives a high quantile to look at (e.g. above 0.9, like in our case in the previous question). What we want to avoid is to go beyond the critical fractile, because in that case we will not choose the optimal quantity, and we will increase the costs and hence decrease the profit, even though this will happen with a low probability (of 1 - q).
This is linkable to the Value at Risk as it is also describing an extreme situation in which the variable takes a value that is higher than a certain threshold, and, as we are in the risk analysis field we are studying the losses on investments, we want to avoid these extreme cases, as they mean a high loss. 

The **Expected Shortfall** at level alpha is the average value of X when it is higher than VaR. It is related to VaR by 

$$ ES_\alpha (x) = \frac{1}{1 - \alpha} \int_\alpha^{1}q_u (F_X)du =  \frac{1}{1 - \alpha} \int_\alpha^{1} VaR_\beta(L)d\beta  $$

The Expected Shortfall at level p will then be the value of the sales if they are higher then the VaR, so for the level p, the ES will be the average value of the sales that are higher than the level p that is calculated as the quantile of the critical fractile q.

# Exercise d

> Fit a Poisson model to the sales volume data by assuming that Yi ∼ Poisson(μ) and using maximum likelihood estimation for μ. (Hint: you may use MASS::fitdistr). According to this model, what is the estimated critical fractile?

```{r poisson by hand}

poisson <- fitdistr(sales$Sales, "Poisson")
#lambda of the poisson distribution
lambda <- poisson$estimate
#quantile 
q <- qpois(p, lambda)

```

In the case of a poisson distribution, we find a critical fractile by using the following formula: 

$$ q = F^{-1} (p) = \sum_{i = 0}^p \frac {e{-\lambda}\lambda^i}{i!}$$

And it corresponds to `r q`. 

#Exercise e

> Suppose we model the sales as an extreme value distribution using a peaks-over-thresholds method. Give the Value at Risk at level p under this model.

```{r EVD, fig.width=12, fig.height=10}

#Mean Residual Life Plot
extRemes::mrlplot(sales$Sales, main="Mean Residual Life Plot")
#Finding and appropriate threshold 
extRemes::threshrange.plot(sales$Sales, r = c(100, 200), nint = 120)

#let's fix the threshold at 140 as after it the distribution seems to be flat in the second graph and there is the elbow in the first one
u <- 190

plot(sales$Sales, type = "l", main = "Daily Sales")
abline(h = u, col = "red")

#Weights for the observations above the threshold chosen
W <-  sales %>% dplyr::filter(Sales > u) %>% dplyr::mutate(W = Sales - u) %>% dplyr::select(W)

#fitting a GPD, method amle as we want the shape parameter positive
gpd <- gPdtest::gpd.fit(W$W, method = "amle")

#extracting the parameters 
#shape
beta <- gpd[1,]
#scale
xi <- gpd[2,]

#setting the alpha for the VaR to the level we have calculated 
alpha <- p

#calculating Fbar with the approximation 
Fbar <- nrow(W) / nrow(sales)

#calculating the values at risk 
VaR <- (u + (beta/xi) * ( ( ( (1 - alpha) / (Fbar) ) ^ (-xi) ) - 1) )

```

We wanted to use the first two graphs to determine the therhsold that we want to use. What we can see in the first one is that between the value 100 and 150 the curve seems to flatten, which is confirmed by the second graphs, as the parameters are becoming constant already from before 100. However, choosing a threshold between these two values would give too many observations above it, in our opinion, hence we decide to move it a little bit further up. We decided to be safe and to choose a threshold of 190, so that we have a quantile that is around the 80%. 

We move on by calculating the weights of the observations that are above this threshold (that are the points in the graph above the red line). 

Then, using these observations we fit a GPD and we estimate the parameters, that we will insert in the formula to calculate the VaR.

The resulting value is `r VaR`. 

# Exercise f

> Perform a binomial back-test over the last 300 days in the dataset for the models in d) and e), using a window size of 365 days. What are your conclusions?

To backtest:
1. Split the data and create a test set including the last 300 observations of the dataset
2. Calculate the VaR for each one of the observations inside the test set using a window of 365 days 
3. Compare the estimated VaR to the real values and see if there is any violation, meaning that the real value is higher than the VaR for a given day
4. Consider that the violations follow a binomial distribution(300, 1 - p), test the hypothesis that the observed proportion of violations over the test set is different from the expected proportion given by the binomial distribution with a binomial test

## Poisson

### 1. Test set 

We will split the data in two subgroups, the first one being the training set which we will use to estimate the VaR will comprehend the observations up to the last 300 days of the data, while the last 300 observations will be the test set to which we will compare the VaR we will find. 

```{r test set poisson}

salesTest <- sales$Sales[(nrow(sales)-299):(nrow(sales))]

```

### 2. Calculate VaR 

Then, we calculate the Value at Risk by fitting a poisson distribution on the window preceeding the observation in which we are interested. 

We start by creating a function that works in the following way:
1. use the same probability as before (hence 0.91)
2. fit the poisson on the window 
3. determines the lambda of the poisson distribution
4. caclulate the VaR by taking the 0.91 quantile of the poisson we fit

```{r poisson VaR}

poissVaR <- function(df){ 
  
  #definition of the probability 
  p <- 0.91
  
  #fitting the poisson
  poisson <- fitdistr(df$Sales, "Poisson")
  
  #lambda of the poisson distribution
  lambda <- poisson$estimate
  
  #quantile, which corresponds to the VaR
  VaRpoiss <- qpois(p, lambda)
}

```

Then, we can create a loop to get the VaR for the observations inside the test set.

```{r VaR calculation poisson}

#create a vector to store the values 
poissVAR <- as.numeric(1:300)

for(i in 1:300){
  #select window 
  wind <- sales %>% dplyr::slice((1913-i):(1913-i-365))
  #calculate VaR
  poissVAR[i] <- poissVaR(wind)
}

poissVAR

```

As we can see they take they almost take the same values, this is due to the fact that the window is quite large (365), hence the change from one window to the subsequent one will not be very big, as we just drop the furthest away observation to gain the most recent one that is the day before the one we are considering. Hence, the fitting of the poisson distribution will be almost the same, as it will be the lambda and the quantile. 

### 3. Violations 

Now, we compare the VaR we estimated to the real values, to see how many violations there are. The should be following a bernoulli distribution with n = 300 and p = 1 - alpha.

```{r comparison poisson}

#number of observed violations
poiss.violations <- sum(ifelse(poissVAR > salesTest, 0, 1))
#number of theoretical violations, given by the mean of the bernoully distribution
ev <- 300*(1 - p)

```

We find out that there are `r poiss.violations` violations. 

We can compare this value to the theoretical one that we expect to get, which is `r ev`.

#### 4. Binomial test

### Binomial test

We can test whether they are statistically significantly different by using a binomial test, which has the following characteristics: 

$$H_0: \pi = \pi_0$$
In our case the theoretical π will equal to `r 1 - p`, while the probability found in the observations is `r poiss.violations/300`. We want to test if waht it observed is different from the expected value. 

```{r binomial test poisson}

(bintest.poiss <- binom.test(poiss.violations, 300, p = 1-p))

```

We can see that for the poisson, the p-value is really low `r bintest.poiss$p.value`. This means that we will reject the null hypothesis and hence that the distribution of the violations is not following a bernoulli distributions. There has been an underestimation of the quantile we used to calculate the VaR and the values at risk, as we had quite a lot violations compared to the theoretical one. We believe that by increasing the probability in the poisson distribution we would get to a more optimal result. 

##Peaks Over Threshold

### 1. Test set

We will split the data in two subgroups, the first one being the training set which we will use to estimate the VaR will comprehend the observations up to the last 300 days of the data, while the last 300 observations will be the test set to which we will compare the VaR we will find. 

```{r test set POT}

salesTest <- sales$Sales[(nrow(sales)-299):(nrow(sales))]

```

### 2. Calculate VaR 

Then we will calculate the VaR for the test set using a moving window of 365 days. 

In order to do so, we start by creating a function to calculate the VaR for a given window, which works in the following way:
- here we use the 73% quantile to determine the threshold as a higher one would mean too few observations above the line to be able to fit a gpd and it would create too few violations, while a lower one would create the opposite problems,
- window of 365 days before the observations that we want to estimate,
- calculate the weights for the observations in this window which are above the threshold by taking the differences between the sales and the thershold,
- fit a GPD on the weights and estimate the parameters for the shape and the scale,
- same alpha as before, which is the p value we calculated,
- estimation of Fbar ny dividing the number of observations above the threshold in the window and the total number of observation in the window,
- put all the estimators in the formula to calculate the value at risk.

```{r VaR function POT}

VaRest <- function(df){
  
  #threshold
  u <- 0.73 * max(df$Sales)
  
  #Weights for the observations above the threshold chosen
  W <-  df %>% dplyr::filter(Sales > u) %>% dplyr::mutate(W = Sales - u) %>% dplyr::select(W)
  
  #fitting a GPD, method amle as we want the shape parameter positive
  gpd <- gPdtest::gpd.fit(W$W, method = "amle")
  
  #extracting the parameters 
  #shape
  beta <- gpd[1,]
  #scale
  xi <- gpd[2,]
  
  #setting the alpha for the VaR to the level we have calculated
  alpha <- p
  
  #calculating Fbar with the approximation 
  Fbar <- nrow(W) / 300
  
  #calculating the values at risk 
  (u + (beta/xi) * ( ( ( (1 - alpha) / (Fbar) ) ^ (-xi) ) - 1) )
  
  }

```

Then we move on with the calculation of the value at risk for each observation in the test set. 
In order to do it, we will first create a vector to store the values, which will have a length of 300 as the number of observations in the test set. 
Then, we create a loop for the 300 observations, we want to create the window considered to calculate the value at risk by including the 365 observations that precedes the one for which we want to estimate the value at risk.
Eventually, we get the different values at risk for each one of the observations. 

```{r VaR calculation POT}

#create a vector to store the values 
VAR <- as.numeric(1:300)

for(i in 1:300){
  #select window 
  wind <- sales %>% dplyr::slice((1913-i):(1913-i-365))
  #calculate VaR
  VAR[i] <- VaRest(wind)
}

VAR

```

We can see that there are quite a lot of repetitions in the estimation of the values at risk, this is given by the fact that the the observations contained in the windows of adjacent dates are all the same but one, as we discard the oldest one to replace it with the real value of the observation of the day before the one we are considering. Hence, if none of these two values is describing a violation (meaning that it is higher than the threshold that we have chosen), the computation of the value at risk won't be affected, as all the paramters that we use to estimate the VaR will remain unchanged: the weights will be the same (since we consider only the values that are above the threshold) and so will be the GPD and its parameters' estimations, the fbar is also the same as Nu stays the same and the alpha is fixed.

### 3. Violations 

Now, we compare the VaR we estimated to the real values, to see how many violations there are. The should be following a bernoulli distribution with n = 300 and p = 1 - alpha.

```{r comparison POT}

#number of observed violations
violations <- sum(ifelse(VAR > salesTest, 0, 1))
#number of theoretical violations, given by the mean of the bernoully distribution
ev <- 300*(1 - p)

```

We find out that there are `r violations` violations. 

We can compare this value to the theoretical one that we expect to get, which is `r ev`.

### Binomial test

We can test whether they are statistically significantly different by using a binomial test, which has the following characteristics: 

$$H_0: \pi = \pi_0$$
In our case the theoretical π will equal to `r 1 - p`, while the probability found in the observations is `r violations/300`. We want to test if waht it observed is different from the expected value. 

```{r binomial test POT}

(bintest <- binom.test(violations, 300, p = 1-p))

```

We can see that the p-value is quite high (`r bintest$p.value`), more specifically is higher than the significance level alpha = 5%, hence we cannot reject the null hypothesis of the two proportions being the same and hence that the distribution of the violations in the windows considered follows a binomial(300, 1-p).
